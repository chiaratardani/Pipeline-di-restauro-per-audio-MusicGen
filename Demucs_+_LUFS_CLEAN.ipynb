{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "outputId": "c0fbf37d-873c-4ede-fae0-8b303af97f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyloudnorm\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm) (1.16.1)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm) (2.0.2)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm) (1.0.0)\n",
            "Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: pyloudnorm\n",
            "Successfully installed pyloudnorm-0.1.1\n",
            "Utilizzo dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "#INIZIALIZZAZIONE AMBIENTE\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import subprocess\n",
        "from torch import nn\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "!pip install pyloudnorm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#CONFIGURAZIONE DEVICE\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilizzo dispositivo: {device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funzione per creare nomi file sicuri--> Questa funzione utility rimuove caratteri speciali dai nomi dei file\n",
        "def create_safe_filename(text, max_length=30):\n",
        "    \"\"\"Crea un nome file sicuro rimuovendo caratteri speciali\"\"\"\n",
        "    #Sostituisci spazi con underscore\n",
        "    safe_text = re.sub(r'\\s+', '_', text)\n",
        "    #Rimuovi caratteri non alfanumerici\n",
        "    safe_text = re.sub(r'[^a-zA-Z0-9_]', '', safe_text)\n",
        "    #Limita la lunghezza(per compatibilt\u00e0 con diversi filesystem)\n",
        "    return safe_text[:max_length]"
      ],
      "metadata": {},
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEXT-CONDITIONAL GENERATION**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIAMO ADESSO LA FUNZIONE 'generate_samples' PER GENERARE I NOSTRI SAMPLES DA MUSICGEN,\n",
        "#SALVANDOLI IN UNA SPECIFICA DIRECTORY\n",
        "\n",
        "#La funzione accetta 2 parametri: la directory di output e il numero di campioni per categoria\n",
        "def generate_samples(output_dir=\"raw_samples\", num_samples_per_category=5): #Voglio generare 5 samples per ogni categoria, per evitare squilibri nel dataset\n",
        "    import time #Viene importato il modulo time per le pause tra le generazioni\n",
        "    os.makedirs(output_dir, exist_ok=True) #Crea la directory di output se non esiste\n",
        "    #Importiamo qui i moduli necessari\n",
        "    from transformers import AutoProcessor, MusicgenForConditionalGeneration #Si caricano i componenti necessari di Hugging Face Transformers\n",
        "    import scipy.io.wavfile as wavfile #Si importa scipy per salvare i file audio\n",
        "\n",
        "    print(\"Caricamento modello MusicGen...\")\n",
        "\n",
        "    # Carica modello e processore\n",
        "    processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\") #Si carica il processore per preparare gli input testuali\n",
        "    model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\") #Si carica il modello MusicGen nella versione \"small\"\n",
        "    model.to(device) #Il modello viene spostato sulla GPU (se disponibile) per l'elaborazione accelerata\n",
        "\n",
        "    #DEFINIAMO LE CATEGORIE E I LORO PROMPTS PER LA GENERAZIONE AUDIO\n",
        "    #Definiamo diverse categorie principali su cui fare testing: All'interno della categoria,\n",
        "    #vengono elencati diversi prompt descrittivi(ogni prompt descrive un tipo specifico di musica o suono).\n",
        "    #Tra le categorie includiamo anche quella del cantato('Musica con voce'), perch\u00e8, nonostante\n",
        "    #MusicGen non sia progettato per generare voci umane realistiche e ci\u00f2 che produce in\n",
        "    #risposta a prompt vocali sono spesso vocalizzi non linguistici o suoni simili a cori,\n",
        "    #\u00e8 rilevante per il nostro progetto includere anche samples di questo tipo.\n",
        "    categories = {\n",
        "        \"Strumenti singoli\": [\n",
        "            \"jazz piano solo\", \"classical guitar piece\",\n",
        "            \"acoustic bass groove\", \"violin sonata\",\n",
        "            \"saxophone improvisation\"\n",
        "\n",
        "        ],\n",
        "        \"Ensemble musicali\": [\n",
        "            \"jazz trio piano bass drums\", \"string quartet classical\",\n",
        "            \"rock band guitar drums bass\",\n",
        "            \"marching band brass percussion\",\n",
        "            \"folk band fiddle banjo guitar\"\n",
        "        ],\n",
        "        \"Generi elettronici\": [\n",
        "            \"techno beat 130 bpm\", \"dubstep wobbly bass\",\n",
        "            \"house music piano\", \"drum and bass fast\",\n",
        "            \"chillout downtempo\"\n",
        "        ],\n",
        "        \"Musica con voce\": [\n",
        "            \"female pop vocal catchy chorus\", \"rap verse with beatbox\",\n",
        "            \"opera tenor aria powerful\", \"male rock vocal gritty\",\n",
        "            \"R&B soulful female vocal\"\n",
        "        ],\n",
        "        \"Effetti\": [\n",
        "            \"epic battle scene with swords and dragons\",\n",
        "            \"rainstorm with thunder and wind\",\n",
        "            \"busy city street with traffic and people\",\n",
        "            \"forest with animals and streams\",\n",
        "            \"spaceship engine and sci-fi effects\"\n",
        "        ],\n",
        "        \"Distorti\": [    #La categoria \"Distorti\" include prompt per audio con distorsioni intenzionali\n",
        "            \"8-bit video game music with intentional glitches\",\n",
        "            \"low quality radio broadcast with noise\",\n",
        "            \"overcompressed pop song with artifacts\",\n",
        "            \"vinyl record with heavy scratches and crackle\",\n",
        "            \"heavily distorted guitar with digital clipping\"\n",
        "\n",
        "        ]\n",
        "      }\n",
        "\n",
        "\n",
        "    #GENERA DATASET: Per ogni categoria e prompt, generiamo un audio sample.\n",
        "    #Per ogni categoria nelle categorie definite: si stampa il nome della categoria e\n",
        "    #si seleziona un sottoinsieme casuale di prompt (il numero specificato o tutti se sono meno)\n",
        "    print(\"Generazione campioni principali...\")\n",
        "    total_samples = 0\n",
        "\n",
        "    for cat_name, prompts in categories.items():\n",
        "        print(f\"\\nGenerando categoria: {cat_name}\")\n",
        "\n",
        "        #Seleziona un sottoinsieme di prompt per questa categoria\n",
        "        selected_prompts = random.sample(prompts, min(num_samples_per_category, len(prompts)))\n",
        "        #Per ogni prompt selezionato: si sceglie una durata casuale tra 5 e 10 secondi e\n",
        "        #si crea un prefisso per il nome del file basato sulla categoria\n",
        "        for prompt in tqdm(selected_prompts, desc=f\"Generating {cat_name}\"):\n",
        "            try:\n",
        "                duration = random.randint(5, 10)\n",
        "                prefix = f\"{cat_name[:3].lower()}_{create_safe_filename(cat_name)[:10]}\" #Si crea un prefisso per il nome del file basato sulla categoria\n",
        "\n",
        "                # Prepara l'input\n",
        "                inputs = processor(    #Il processore trasforma il prompt testuale in un formato che il modello pu\u00f2 comprendere\n",
        "                    text=[prompt],\n",
        "                    padding=True,\n",
        "                    return_tensors=\"pt\", #Il testo viene convertito in tensori PyTorch\n",
        "                ).to(device)           #I tensori vengono spostati sul dispositivo appropriato (GPU/CPU)\n",
        "\n",
        "                #USIAMO IL MODELLO PER GENERARE AUDIO DAL PROMPT TESTUALE\n",
        "                audio_values = model.generate(\n",
        "                    **inputs,  #Il modello genera audio basato sul prompt testuale\n",
        "                    max_new_tokens=int(duration * 50),  #max_new_tokens controlla la lunghezza dell'audio generato (circa 50 token al secondo)\n",
        "                    do_sample=True    #do_sample=True abilita la generazione casuale invece di una generazione deterministica\n",
        "                )\n",
        "                #ORA SALVIAMO L'AUDIO GENERATO COME WAV FILE\n",
        "                #Estrai l'audio e converti in numpy# MusicGen usa 32kHz--> Il sample rate \u00e8 fissato a 32kHz (valore predefinito di MusicGen), anche se in\n",
        "                audio = audio_values[0, 0].cpu().numpy()  #L'audio generato viene estratto dai tensori PyTorch: viene convertito in un array NumPy per l'elaborazione successiva\n",
        "                #MusicGen usa 32kHz--> Il sample rate \u00e8 fissato a 32kHz (valore predefinito di MusicGen), anche se,\n",
        "                #in realt\u00e0, Demucs \u00e8 stato ottimizzato per sample rate pari a 44.1 kHz, che \u00e8\n",
        "                #anche lo standard per l'industria musicale(molti software e dispositivi si\n",
        "                #aspettano audio a 44.1 kHz)\n",
        "                sr = 32000\n",
        "\n",
        "                #Creazione filename sicuro--> Il nome del file viene reso \"sicuro\" rimuovendo caratteri speciali\n",
        "                safe_prompt = create_safe_filename(prompt)\n",
        "                filename = f\"{prefix}_{safe_prompt}.wav\"\n",
        "                filepath = os.path.join(output_dir, filename) #Il file viene salvato nella directory di output\n",
        "\n",
        "                #Salva con scipy per evitare problemi(per evitare problemi di compatibilit\u00e0 con altre librerie)\n",
        "                #con torchaudio/soundfile\n",
        "                wavfile.write(filepath, sr, audio)\n",
        "\n",
        "                total_samples += 1 #Il contatore dei campioni totali viene incrementato\n",
        "                torch.cuda.empty_cache() #La cache della GPU viene liberata dopo ogni generazione per prevenire problemi di memoria\n",
        "\n",
        "                #Pausa breve per evitare sovraccarico della GPU: viene inserita una pausa di 1 secondo per evitare sovraccarico della GPU\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e: #Gli errori vengono catturati e visualizzati senza interrompere l'intero processo\n",
        "                print(f\"Errore generazione {prompt}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\nTotale campioni generati: {total_samples}\") #Alla fine, viene stampato il numero totale di campioni generati"
      ],
      "metadata": {},
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CARICAMENTO DEL MODELLO DEMUCS**\n",
        "\n",
        "Demucs \u00e8 un modello di AI generativa sviluppato con lo scopo di separare un audio negli stems che lo costituiscono, come voce, percussioni, bassi e altri strumenti. Demucs viene principalmente utilizzato nella produzione e ricerca musicale, permettendo una vasta gamma di operazioni, quali remixing, sampling e la generazione anche di audio destinati al karaoke.\n",
        "\n",
        "Nonostante i suoi sorprendenti utilizzi, Demucs e il suo design transformer-based presentano delle limitazioni. I transformers infatti richiedono un'ingente quantit\u00e0 di labeled data per ottenere risultati ottimali; inoltre, modelli sperimentali a 6 sorgenti (con l'aggiunta di piano e chitarra) presentano importanti distorsioni e artefatti, in particolare per quanto riguarda la sorgente del piano."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIAMO ORA LA FUNZIONE 'load_demucs_' che carica il modello Demucs e la funzione apply_model\n",
        "def load_demucs(): #Questa funzione importa le funzioni necessarie dalla libreria Demucs.\n",
        "    from demucs.pretrained import get_model\n",
        "    from demucs.apply import apply_model\n",
        "    print(\"Caricamento modello Demucs...\")\n",
        "    #Mentre htdemucs(trained on MusDB + 800 songs) \u00e8 la prima versione di Hybrid Trasformer Demucs, htdemucs_ft ne \u00e8 la\n",
        "    #versione fine-tuned.\n",
        "    model = get_model('htdemucs_ft')\n",
        "    #Il modello viene spostato sul device specificato(GPU se \u00e8 disponibile) e impostato in modalit\u00e0 'valutazione'\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, apply_model #Ritorniamo il modello e la funzione apply_model\n"
      ],
      "metadata": {},
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PIPELINE DI RESTAURO**\n",
        "\n",
        "La pipeline qui implementata prevede l'uso di Demucs, seguito dalla normalizzazione LUFS."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINIAMO LA CLASSE 'DemucsRestorer' CHE USA IL MODELLO CARICATO PER LA RESTAURAZIONE AUDIO\n",
        "class DemucsRestorer:\n",
        "    def __init__(self, sample_rate=44100): #La classe viene inizializzata con un sample rate(default 44100 Hz)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.demucs, self.apply_demucs = load_demucs() #Nel metodo __init__, carichiamo il modello Demucs e la funzione apply_model  richiamando la funzione 'load_demucs'\n",
        "\n",
        "        #Pesi fissi per la fusione: Settiamo pesi fissi per le quattro sorgenti(vocals, bass, drums, other)\n",
        "        #sotto forma di tensore [0.25, 0.25, 0.25, 0.25],\n",
        "        #ovvero settiamo pesi uguali per ogni sorgente.\n",
        "        #Assicuriamoci che i pesi siano sul device corretto e che siano 3D per\n",
        "        #il broadcasting con (sources,channels,samples).\n",
        "        self.source_weights = torch.tensor([0.25, 0.25, 0.25, 0.25], device=device).view(4, 1, 1) #Il tensore ha una shape (4, 1, 1)\n",
        "\n",
        "    def enhance(self, audio): #Ha come input un tensore la cui shape dovrebbe essere (channels,samples) dopo il preprocessing.\n",
        "        print(f\"Enhance input shape: {audio.shape}\") #Printiamo la shape dell'input.\n",
        "\n",
        "        #Assicuriamoci che l'audio sia sul device corretto e in float32.\n",
        "        audio = audio.to(device).float()\n",
        "\n",
        "        #Demucs si aspetta (batch, channels, samples)\n",
        "        audio_batch = audio.unsqueeze(0) #Aggiungiamo una dimensione batch('unsqueezing' the tensor), perch\u00e8 Demucs si aspetta (batch,channels,samples)\n",
        "\n",
        "        print(f\"Shape before apply_demucs: {audio_batch.shape}\") #Printiamo la shape dopo aver aggiunto una dimensione batch\n",
        "\n",
        "        #SEPARAZIONE DELLE SORGENTI CON DEMUCS(Abbiamo disabilitato il calcolo dei gradienti (per risparmiare memoria durante l'inferenza))\n",
        "        with torch.no_grad(): #Applichiamo il modello Demucs per separare l'audio in 4 sorgenti(vocals, bass, drums, other) senza gradiente.\n",
        "            #Passiamo audio_batch\n",
        "            #apply_model si aspetta infatti un audio dalla shape (batch, channels, samples)\n",
        "            sources = self.apply_demucs(self.demucs, audio_batch, device=device)\n",
        "            #Le sorgenti in output divrebbero essere del tipo(batch, sources, channels, samples)\n",
        "            print(f\"Sources shape after apply_demucs: {sources.shape}\")\n",
        "\n",
        "            #L'output di apply_model ha shape(batch, sources, channels, samples). Rimuoviamo dunque\n",
        "            #la dimensione batch('squeezing' the tensor) per ottenere (sources, channels, samples).\n",
        "            sources = sources.squeeze(0)\n",
        "            print(f\"Sources shape after squeeze batch: {sources.shape}\") #Printiamo la shape dopo aver rimosso la dimensione batch\n",
        "\n",
        "        #FUSIONE CON I PESI FISSI\n",
        "        #sources shape: (sources, channels, samples)\n",
        "        #self.source_weights shape: (sources, 1, 1)\n",
        "        weighted_sources = sources * self.source_weights #Moltiplica ogni sorgente per il suo peso(mediante broadcasting)\n",
        "        combined = weighted_sources.sum(dim=0) #Sommiamo ora tutte le sorgenti ponderate lungo la dimensione delle sorgenti, per ottenere un audio combinato del tipo (channels,samples)\n",
        "        print(f\"Combined shape after sum: {combined.shape}\")\n",
        "\n",
        "        #Convertiamo l'audio combinato a mono facendo la media lungo i channels, ottenendo una shape(samples,).\n",
        "        combined_mono = combined.mean(dim=0)\n",
        "        print(f\"Combined mono shape: {combined_mono.shape}\")\n",
        "\n",
        "        #Assicuriamoci che l'output sia 2D (1, samples) per salvare l'audio con torchaudio\n",
        "        result = combined_mono.unsqueeze(0)\n",
        "        print(f\"Final result shape: {result.shape}\")\n",
        "\n",
        "        return result  #Ritorna il risultato, che \u00e8 l'audio restaurato mediante Demucs mixing."
      ],
      "metadata": {},
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NORMALIZZAZIONE LUFS(che verr\u00e0 applicata poi, come postprocessing, dopo aver restaurato l'audio con Demucs)\n",
        "#Questa funzione normalizza il volume dell'audio in output per matchare quello dell'audio originale\n",
        "#utilizzando lo standard LUFS(Loudness Units Full Scale), standard industriale per misurare la loudness percepita.\n",
        "def normalize_loudness_lufs(input_audio, output_audio, sample_rate):\n",
        "    try:\n",
        "        import pyloudnorm as pyln\n",
        "\n",
        "        #Converti i tensori input ed output in array numpy e computa le loro\n",
        "        #trasposte, perch\u00e8 pyloudnorm si aspetta (samples, channels)\n",
        "        input_np = input_audio.cpu().numpy().T\n",
        "        output_np = output_audio.cpu().numpy().T\n",
        "\n",
        "        #Crea misuratore LUFS dato il sample_rate\n",
        "        meter = pyln.Meter(sample_rate)\n",
        "\n",
        "        #Misura loudness originale: calcola la loudness integrata (media su tutto il segnale) dell'audio originale.\n",
        "        input_loudness = meter.integrated_loudness(input_np)\n",
        "\n",
        "        #Applica normalizzazione: normalizziamo l'audio in output affinch\u00e8 abbia la stessa\n",
        "        #loudness integrata dell'audio in input.\n",
        "        output_normalized = pyln.normalize.loudness(output_np,\n",
        "                                                   meter.integrated_loudness(output_np),\n",
        "                                                   input_loudness)\n",
        "\n",
        "        return torch.from_numpy(output_normalized.T).float() #Converti l'audio normalizzato ad un tensore torch e ritornalo\n",
        "\n",
        "    except Exception as e: #Se c'\u00e8 un errore durante il processo, lo printa e ritorna l'audio in output originale\n",
        "        print(f\"Errore nella normalizzazione LUFS: {str(e)}\")\n",
        "        return output_audio"
      ],
      "metadata": {},
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PIPELINE DI RESTAURO\n",
        "class AudioRestorationPipeline: #Questa classe gestisce il processo di restaurazione audio per file individuali e directories.\n",
        "\n",
        "    def __init__(self, sample_rate=44100): #Metodo __init__ :Inizializza la pipeline con un sample rate(default 44100 Hz) e crea un'istanza del restorer Demucs\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.model = DemucsRestorer(sample_rate)\n",
        "\n",
        "    def enhance_audio(self, input_path, output_path): #Questo metodo processa un singolo file audio\n",
        "        try:\n",
        "        #Caricamento audio con torchaudio\n",
        "            audio_tensor, sr = torchaudio.load(input_path)\n",
        "            print(f\"Loaded audio shape: {audio_tensor.shape}, sample rate: {sr}\")\n",
        "\n",
        "        #Salva l'audio originale per la normalizzazione LUFS\n",
        "            original_audio = audio_tensor.clone()\n",
        "\n",
        "        #PREPROCESSING:Il preprocessing garantisce che Demucs riceva sempre audio stereo\n",
        "        #Se l'audio \u00e8 mono(1 channel), convertilo a stereo copiando e ripetendo il channel;\n",
        "        #se l'audio ha pi\u00f9 di due channels(multichannel), prendi solo i primi due.\n",
        "            if audio_tensor.shape[0] == 1:\n",
        "              audio_tensor = audio_tensor.repeat(2, 1)\n",
        "            elif audio_tensor.shape[0] > 2:\n",
        "              audio_tensor = audio_tensor[:2, :]\n",
        "            print(f\"Audio shape after preprocessing: {audio_tensor.shape}\")\n",
        "\n",
        "            #Normalizzazione\n",
        "            max_val = torch.max(torch.abs(audio_tensor))\n",
        "            if max_val < 1e-6: #Se il valore massimo dell'audio \u00e8 molto basso, salta l'elaborazione per evitare errori.\n",
        "              print(f\"Warning: Audio file {input_path} is silent or near silent. Skipping enhancement.\")\n",
        "              return\n",
        "\n",
        "           #Spostiamo il tensore audio sul device appropriato prima di fare resampling\n",
        "            audio_tensor = audio_tensor.to(device)\n",
        "\n",
        "            #Facciamo resampling se necessario, ovvero se il sample rate dell'audio\n",
        "            #non corrisponde al sample rate target(default 44100 Hz).\n",
        "            if sr != self.sample_rate:\n",
        "                print(f\"Resampling from {sr}Hz to {self.sample_rate}Hz\")\n",
        "                resampler = T.Resample(sr, self.sample_rate).to(device)\n",
        "                audio_tensor = resampler(audio_tensor)\n",
        "                print(f\"Shape after resampling: {audio_tensor.shape}\")\n",
        "\n",
        "            #RESTAURAZIONE\n",
        "            enhanced = self.model.enhance(audio_tensor)\n",
        "\n",
        "\n",
        "            #NORMALIZZAZIONE LUFS\n",
        "            enhanced_normalized = normalize_loudness_lufs(\n",
        "            original_audio,\n",
        "            enhanced.detach().cpu(),   #detach().cpu() sposta l'audio dalla GPU alla CPU e lo scollega dal grafo computazionale\n",
        "            self.sample_rate\n",
        "            )\n",
        "        #Salvataggio\n",
        "            print(f\"Enhanced shape before save: {enhanced_normalized.shape}\")\n",
        "\n",
        "        #Assicurati che l'output sia 2D (channels,samples) per il salvataggio con torchaudio\n",
        "            if enhanced_normalized.dim() == 1: #Se l'audio \u00e8 mono (1D), aggiunge una dimensione canale per renderlo 2D\n",
        "                enhanced_normalized = enhanced_normalized.unsqueeze(0)\n",
        "\n",
        "        #Crea una directory di output se non esiste\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        #Salviamo con torchaudio.save\n",
        "            torchaudio.save(output_path, enhanced_normalized, self.sample_rate)\n",
        "            print(f\"Processato e salvato: {os.path.basename(input_path)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore durante l'elaborazione o il salvataggio di {input_path}: {str(e)}\")\n",
        "\n",
        "\n",
        "    #Questo metodo invece processa tutti i file WAV nell'input directory che\n",
        "    #non si trovano nella output directory, ovvero che non sono ancora stati restaurati.\n",
        "    def process_directory(self, input_dir, output_dir):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        #Processa soltanto i file che non hanno un file corrispondente\n",
        "        #nella directory di output. Individuiamo dunque tutti i file WAV che\n",
        "        #sono nella input e nell'output directory.\n",
        "        input_files = [f for f in os.listdir(input_dir) if f.endswith('.wav')]\n",
        "        output_files = os.listdir(output_dir)\n",
        "        files_to_process = [f for f in input_files if f not in output_files] #Identifica solo i file che non hanno una controparte gi\u00e0 processata nella directory di output.\n",
        "\n",
        "        if not files_to_process: #Se non ci sono file da processare, printiamo un messaggio\n",
        "            print(\"Nessun nuovo file da elaborare nella directory di input.\")\n",
        "            print(f\"File gi\u00e0 elaborati: {len(input_files)}\")\n",
        "            return\n",
        "\n",
        "        #Altrimenti, processiamo questi file 'mancanti' mediante 'enhance_audio'\n",
        "        for file in tqdm(files_to_process, desc=\"Processing audio\"):\n",
        "            input_path = os.path.join(input_dir, file)\n",
        "            output_path = os.path.join(output_dir, file)\n",
        "            self.enhance_audio(input_path, output_path)\n",
        "\n",
        "        print(f\"\\nCompletata elaborazione. File in output directory: {len(os.listdir(output_dir))}\")"
      ],
      "metadata": {},
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VALUTAZIONE AUDIO**\n",
        "\n",
        "Definiamo tre metriche: improved Signal-to-Noise Ratio (iSNR), High-Frequency Energy Ratio, e Spectral Contrast Improvement."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#VALUTAZIONE AUDIO\n",
        "def calculate_audio_metrics(input_path, output_path):\n",
        "    try:\n",
        "        #Carica con torchaudio: carica sia l'audio originale che quello processato usando torchaudio e ottiene i sample rate di entrambi i file\n",
        "        input_audio, sr_in = torchaudio.load(input_path)\n",
        "        output_audio, sr_out = torchaudio.load(output_path)\n",
        "\n",
        "        #Converte l'audio multicanale in mono facendo la media tra i canali (questo semplifica l'analisi successiva)\n",
        "        if input_audio.shape[0] > 1:\n",
        "            input_audio = torch.mean(input_audio, dim=0)\n",
        "        if output_audio.shape[0] > 1:\n",
        "            output_audio = torch.mean(output_audio, dim=0)\n",
        "\n",
        "        #Se i sample rate sono diversi, facciamo resampling dell'audio processato per matchare quello originale\n",
        "        if sr_in != sr_out:\n",
        "             print(f\"Warning: Sample rates differ ({sr_in} vs {sr_out}) for {os.path.basename(input_path)}. Resampling output to match input.\")\n",
        "             resampler = T.Resample(sr_out, sr_in)\n",
        "             output_audio = resampler(output_audio)\n",
        "             sr_out = sr_in #Aggiorna il sample rate\n",
        "\n",
        "\n",
        "        #Allineamento della lunghezza--> Assicura stessa lunghezza\n",
        "        min_len = min(input_audio.shape[0], output_audio.shape[0])\n",
        "        #Tronca entrambi i segnali alla lunghezza minima tra i due--> Garantisce che i segnali abbiano la stessa durata per un confronto accurato\n",
        "        input_audio = input_audio[:min_len]\n",
        "        output_audio = output_audio[:min_len]\n",
        "\n",
        "        #Converte i tensori PyTorch in array NumPy per l'analisi con librosa\n",
        "        input_audio_np = input_audio.numpy()\n",
        "        output_audio_np = output_audio.numpy()\n",
        "\n",
        "        #Assicura che gli array siano monodimensionali\n",
        "        if input_audio_np.ndim > 1:\n",
        "            input_audio_np = input_audio_np.squeeze()\n",
        "        if output_audio_np.ndim > 1:\n",
        "            output_audio_np = output_audio_np.squeeze()\n",
        "\n",
        "        #Esegue un ulteriore controllo e troncamento per garantire forme identiche\n",
        "        if input_audio_np.shape != output_audio_np.shape:\n",
        "             min_len_np = min(input_audio_np.shape[0], output_audio_np.shape[0])\n",
        "             input_audio_np = input_audio_np[:min_len_np]\n",
        "             output_audio_np = output_audio_np[:min_len_np]\n",
        "             print(f\"Adjusted numpy shapes for {os.path.basename(input_path)}: {input_audio_np.shape} vs {output_audio_np.shape}\")\n",
        "\n",
        "\n",
        "        #CALCOLO iSNR\n",
        "        #Assicura che le shapes siano compatibili per la sottrazione\n",
        "        if input_audio_np.shape != output_audio_np.shape:\n",
        "             print(f\"Error: Input and output audio numpy shapes mismatch after final trimming for {os.path.basename(input_path)}: {input_audio_np.shape} vs {output_audio_np.shape}\")\n",
        "             return {\n",
        "                'iSNR': 0,\n",
        "                'HF_Energy_Ratio': 1,\n",
        "                'Spectral_Contrast_Improvement': 1,\n",
        "             }\n",
        "\n",
        "        #Il \"rumore\" \u00e8 la differenza tra originale e processato--> Questo misura\n",
        "        #quanto il processo di restauro ha alterato il segnale originale:\n",
        "        #nota dunque che la differenza tra l'originale e il processato potrebbe\n",
        "        #includere non solo rumore, ma anche artefatti di separazione o\n",
        "        #cambiamenti nella bilanciatura delle frequenze.\n",
        "        noise = input_audio_np - output_audio_np\n",
        "        #Potenza del segnale originale\n",
        "        signal_power = np.mean(input_audio_np**2)\n",
        "        #Potenza del rumore (differenza)\n",
        "        noise_power = np.mean(noise**2)\n",
        "        #iSNR in dB (nota che l'aggiunta di 1e-10 (un numero molto piccolo)\n",
        "        #previene la divisione per zero, che sarebbe matematicamente indefinita.\n",
        "        #Questo \u00e8 un esempio di \"regolarizzazione\" che stabilizza numericamente il calcolo.)\n",
        "        final_isnr = 10 * np.log10(signal_power / (noise_power + 1e-10)) if noise_power > 0 else float('inf') #Quando noise_power \u00e8 uguale a zero, l'isnr va ad infinito\n",
        "\n",
        "        #CALCOLA L'ENERGIA NELLE ALTE FREQUENZE\n",
        "        def calc_hf_energy(signal, sr):\n",
        "            n_fft = 2048\n",
        "            hop_length = 512\n",
        "            if signal.ndim > 1:\n",
        "                 signal = signal.squeeze()\n",
        "            if signal.ndim == 0: # Gestione del segnale vuoto: se il segnale \u00e8 vuoto dopo lo squeeze, restituisce 0 per evitare errori.\n",
        "                 return 0.0\n",
        "\n",
        "            D = np.abs(librosa.stft(signal, n_fft=n_fft, hop_length=hop_length))\n",
        "            freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
        "            freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
        "            hf_min = 0.4 * (sr/2) #Per generare una soglia relativa\n",
        "            hf_mask = freqs >= hf_min #Crea una maschera booleana che identifica le frequenze superiori a hf_min.\n",
        "            #Verifica che le dimensioni della STFT corrispondano all'array di frequenze(per prevenire errori).\n",
        "            if D.shape[0] != len(freqs):\n",
        "                 print(f\"Warning: STFT frequency bins ({D.shape[0]}) do not match frequency array length ({len(freqs)}) for HF energy calculation.\")\n",
        "                 return 0.0\n",
        "            return np.sum(D[hf_mask, :])\n",
        "\n",
        "        hf_energy_in = calc_hf_energy(input_audio_np, sr_in)\n",
        "        hf_energy_out = calc_hf_energy(output_audio_np, sr_out)\n",
        "        #Gestisce anche la divisione per zero(se hf_energy_in \u00e8 zero) nel\n",
        "        #calcolo del rapporto tra l'energia nelle alte frequenze dell'audio processato e\n",
        "        #quello originale--> Un valore > 1 indica un miglioramento nelle alte frequenze.\n",
        "        hf_energy_ratio = hf_energy_out / (hf_energy_in + 1e-10) if hf_energy_in > 0 else float('inf')\n",
        "\n",
        "        #CALCOLA MIGLIORAMENTO DEL CONTRASTO SPETTRALE:Il contrasto spettrale\n",
        "        #quantifica la differenza tra picchi e valli nello spettro di frequenza.\n",
        "        def calc_spectral_contrast(signal, sr):\n",
        "            #Assicurati che il segnale sia 1D per librosa\n",
        "            if signal.ndim > 1:\n",
        "                 signal = signal.squeeze()\n",
        "            if signal.ndim == 0:\n",
        "                 return 0.0\n",
        "\n",
        "            #Estrae il contrasto spettrale usando librosa.\n",
        "            #Assicura che n_fft e hop_lenght siano compatibili con la lunghezza del segnale.\n",
        "            n_fft = 2048\n",
        "            hop_length = 512\n",
        "            if len(signal) < n_fft: #Se il segnale \u00e8 troppo corto per il valore n_fft di default\n",
        "                 n_fft = len(signal) #Imposta n_fft uguale alla lunghezza del segnale\n",
        "                 hop_length = n_fft // 4 #Sistema hop_lenght\n",
        "                 if hop_length == 0: #Previene hop_lenght dall'essere zero nel caso di segnali molto corti\n",
        "                     hop_length = 1\n",
        "                 print(f\"Warning: Signal too short for default n_fft. Using n_fft={n_fft}, hop_length={hop_length} for spectral contrast.\")\n",
        "\n",
        "            S = np.abs(librosa.stft(signal, n_fft=n_fft, hop_length=hop_length))\n",
        "\n",
        "            if S.shape[1] == 0:\n",
        "                 print(\"Warning: STFT resulted in zero frames. Cannot calculate spectral contrast.\")\n",
        "                 return 1.0 #Ritorna 1.0(=nessun cambiamento) se il contrasto non pu\u00f2 essere calcolato\n",
        "\n",
        "            contrast = librosa.feature.spectral_contrast(S=S, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
        "            return np.mean(contrast)  #Media su tutte le bande e il tempo\n",
        "\n",
        "        spectral_contrast_in = calc_spectral_contrast(input_audio_np, sr_in)\n",
        "        spectral_contrast_out = calc_spectral_contrast(output_audio_np, sr_out)\n",
        "        #Gestisci la divisione per zero(quando spectral_contrast_in \u00e8 zero o \u00e8 molto piccolo).\n",
        "        #Calcola il miglioramento del contrasto spettrale tra audio processato e originale--> Un valore > 1 indica un miglioramento nella chiarezza del suono\n",
        "        spectral_contrast_improvement = spectral_contrast_out / (spectral_contrast_in + 1e-10) if spectral_contrast_in > 0 else float('inf')\n",
        "\n",
        "\n",
        "        return {  #Restituisce un dizionario con tutte e tre le metriche calcolate\n",
        "\n",
        "            'iSNR_final': final_isnr,\n",
        "            'HF_Energy_Ratio': hf_energy_ratio,\n",
        "            'Spectral_Contrast_Improvement': spectral_contrast_improvement,\n",
        "\n",
        "        }\n",
        "    except Exception as e:   #Se qualcosa va storto durante il calcolo delle metriche, cattura l'errore e restituisce valori di default che indicano \"nessun miglioramento\"\n",
        "        print(f\"Errore valutazione: {str(e)}\")\n",
        "        return {\n",
        "\n",
        "            'iSNR_final': 0,\n",
        "            'HF_Energy_Ratio': 1,\n",
        "            'Spectral_Contrast_Improvement': 1,\n",
        "\n",
        "        }"
      ],
      "metadata": {},
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESECUZIONE PRINCIPALE**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#ESECUZIONE PRINCIPALE\n",
        "def main():\n",
        "    #Installa dipendenze mancanti\n",
        "    try:\n",
        "        import demucs\n",
        "        import librosa\n",
        "        from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "        import scipy.io.wavfile\n",
        "        import time  #Per le pause\n",
        "\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"Installazione dipendenze necessarie...\")\n",
        "        subprocess.run([\"pip\", \"install\", \"demucs\", \"librosa\", \"transformers\", \"torchaudio\", \"scipy\"])\n",
        "        import demucs\n",
        "        import librosa\n",
        "        from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
        "        import scipy.io.wavfile\n",
        "        import time\n",
        "\n",
        "    #Configurazione percorsi\n",
        "    RAW_DIR = \"raw_samples\"\n",
        "    RESTORED_DIR = \"restored_samples\"\n",
        "\n",
        "    #Numero di campioni per categoria\n",
        "    NUM_SAMPLES_PER_CATEGORY = 5\n",
        "\n",
        "    #Sample rate desiderato: Possiamo inserire il sample rate desiderato(qui\n",
        "    #abbiamo imposto 32 kHz perch\u00e8 vogliamo sia coerente con gli audio generati da MusicGen,\n",
        "    #ma possiamo anche impostarlo a 44.1 kHz, che \u00e8 lo standard).\n",
        "    DESIRED_SAMPLE_RATE = 32000\n",
        "\n",
        "    #Genera campioni se necessario\n",
        "    if not os.path.exists(RAW_DIR) or len(os.listdir(RAW_DIR)) < NUM_SAMPLES_PER_CATEGORY * 4:\n",
        "        print(\"Generazione campioni MusicGen...\")\n",
        "        generate_samples(RAW_DIR, NUM_SAMPLES_PER_CATEGORY)\n",
        "    else:\n",
        "        print(f\"Trovati {len(os.listdir(RAW_DIR))} campioni esistenti in {RAW_DIR}\")\n",
        "\n",
        "    #Inizializzazione pipeline\n",
        "    print(\"Inizializzazione pipeline di restauro...\")\n",
        "    restoration_pipeline = AudioRestorationPipeline(sample_rate=DESIRED_SAMPLE_RATE)\n",
        "\n",
        "    #Elaborazione audio\n",
        "    print(\"\\nAvvio restauro campioni...\")\n",
        "    restoration_pipeline.process_directory(RAW_DIR, RESTORED_DIR)\n",
        "\n",
        "    #Valutazione risultati: mostra tutti i campioni\n",
        "    if os.path.exists(RESTORED_DIR) and len(os.listdir(RESTORED_DIR)) > 0:\n",
        "        #Prendi tutti i file nella directory RAW_DIR che hanno un corrispondente in RESTORED_DIR\n",
        "        raw_files = [f for f in os.listdir(RAW_DIR) if f.endswith('.wav')]\n",
        "        restored_files = [f for f in os.listdir(RESTORED_DIR) if f.endswith('.wav')]\n",
        "\n",
        "        #Considera solo i file che sono presenti in entrambe le directory\n",
        "        common_files = [f for f in raw_files if f in restored_files]\n",
        "\n",
        "        if not common_files:\n",
        "            print(\"Nessun file comune tra RAW_DIR e RESTORED_DIR per la valutazione.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"VALUTAZIONE RISULTATI - TUTTI I CAMPIONI\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        all_metrics = []\n",
        "        for sample_file in common_files:\n",
        "            input_path = os.path.join(RAW_DIR, sample_file)\n",
        "            output_path = os.path.join(RESTORED_DIR, sample_file)\n",
        "\n",
        "            if os.path.exists(output_path):\n",
        "                metrics = calculate_audio_metrics(input_path, output_path)\n",
        "                all_metrics.append(metrics)\n",
        "\n",
        "                #Estrai categoria dal nome del file\n",
        "                category = sample_file.split('_')[1] if '_' in sample_file else \"Unknown\"\n",
        "\n",
        "                print(f\"\\n{os.path.basename(sample_file)} (Categoria: {category}):\")\n",
        "                print(f\"  \u2022 iSNR finale : {metrics['iSNR_final']:.2f} dB\")\n",
        "                print(f\"  \u2022 Ratio energia alte frequenze: {metrics['HF_Energy_Ratio']:.2f}x\")\n",
        "                print(f\"  \u2022 Miglioramento contrasto spettrale: {metrics['Spectral_Contrast_Improvement']:.2f}x\")\n",
        "            else:\n",
        "                print(f\"\\nFile di output mancante per: {sample_file}\")\n",
        "\n",
        "        #Media delle metriche\n",
        "        if all_metrics:\n",
        "            avg_isnr = np.mean([m['iSNR_final'] for m in all_metrics])\n",
        "            avg_hf_ratio = np.mean([m['HF_Energy_Ratio'] for m in all_metrics])\n",
        "            avg_contrast = np.mean([m['Spectral_Contrast_Improvement'] for m in all_metrics])\n",
        "\n",
        "            print(\"\\n\" + \"-\"*60)\n",
        "            print(f\"MEDIA SU {len(all_metrics)} CAMPIONI:\")\n",
        "            print(f\"  \u2022 SNR migliorato medio: {avg_isnr:.2f} dB\")\n",
        "            print(f\"    \u2022 Ratio HF medio: {avg_hf_ratio:.2f}x\")\n",
        "            print(f\"    \u2022 Miglioramento contrasto spettrale medio: {avg_contrast:.2f}x\")\n",
        "\n",
        "            #Calcola medie per categoria\n",
        "            print(\"\\n\" + \"-\"*60)\n",
        "            print(\"MEDIE PER CATEGORIA:\")\n",
        "\n",
        "            #Raggruppa per categoria\n",
        "            categories_metrics = {}\n",
        "            for i, sample_file in enumerate(common_files):\n",
        "                category = sample_file.split('_')[1] if '_' in sample_file else \"Unknown\"\n",
        "                if category not in categories_metrics:\n",
        "                    categories_metrics[category] = []\n",
        "                categories_metrics[category].append(all_metrics[i])\n",
        "\n",
        "            #Stampa medie per categoria\n",
        "            for category, metrics_list in categories_metrics.items():\n",
        "                cat_isnr = np.mean([m['iSNR_final'] for m in metrics_list])\n",
        "                cat_hf_ratio = np.mean([m['HF_Energy_Ratio'] for m in metrics_list])\n",
        "                cat_contrast = np.mean([m['Spectral_Contrast_Improvement'] for m in metrics_list])\n",
        "\n",
        "\n",
        "                categories_metrics[category] = {\n",
        "                    'iSNR': cat_isnr,\n",
        "                    'HF_Ratio': cat_hf_ratio,\n",
        "                    'Contrast': cat_contrast\n",
        "                }\n",
        "\n",
        "\n",
        "                print(f\"\\n  {category}:\")\n",
        "                print(f\"    \u2022 SNR migliorato medio: {cat_isnr:.2f} dB\")\n",
        "                print(f\"    \u2022 Ratio HF medio: {cat_hf_ratio:.2f}x\")\n",
        "                print(f\"    \u2022 Miglioramento contrasto medio: {cat_contrast:.2f}x\")\n",
        "\n",
        "            #Interpretazione dei risultati\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"INTERPRETAZIONE RISULTATI:\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"\u2022 iSNR > 0 dB: Riduzione del rumore\")\n",
        "            print(\"\u2022 Ratio HF > 1.0: Miglioramento alte frequenze\")\n",
        "            print(\"\u2022 Contrasto > 1.0: Miglioramento chiarezza strumentale\")\n",
        "\n",
        "        #Analisi percettiva aggiuntiva\n",
        "        print(\"\\nAscolta i campioni in:\")\n",
        "        print(f\"  Input: {os.path.abspath(RAW_DIR)}\")\n",
        "        print(f\"  Output: {os.path.abspath(RESTORED_DIR)}\")\n",
        "    else:\n",
        "        print(\"Errore: nessun file restaurato trovato\")\n"
      ],
      "metadata": {},
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "outputId": "36373264-722f-43d8-e002-425299e56322"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installazione dipendenze necessarie...\n",
            "Generazione campioni MusicGen...\n",
            "Caricamento modello MusicGen...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1861837d1e29495498f3ea7f0c21c1ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66050458bbf948e091c42fb89ca8950e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8aeae0106669478391e57e9eb75f5239"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa4caa23916f4388a4821b82f2c6b5e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32c382acd6734c8b883ce8c58e923383"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1877babba71c469e9f70ff766760615e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af2171c784874f5fbc360d3d44d03d2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77c7e70e015c4c23b06cc09234763c1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.36G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06cb419db1224312891820e1c1a79544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a69f39af583f45608f1013972e08d9d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generazione campioni principali...\n",
            "\n",
            "Generando categoria: Strumenti singoli\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Strumenti singoli: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:53<00:00, 10.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generando categoria: Ensemble musicali\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Ensemble musicali: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:44<00:00,  8.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generando categoria: Generi elettronici\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Generi elettronici: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:47<00:00,  9.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generando categoria: Musica con voce\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Musica con voce: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:52<00:00, 10.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generando categoria: Effetti\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Effetti: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:43<00:00,  8.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generando categoria: Distorti\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Distorti: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:52<00:00, 10.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Totale campioni generati: 30\n",
            "Inizializzazione pipeline di restauro...\n",
            "Caricamento modello Demucs...\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/demucs/hybrid_transformer/f7e0c4bc-ba3fe64a.th\" to /root/.cache/torch/hub/checkpoints/f7e0c4bc-ba3fe64a.th\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80.2M/80.2M [00:01<00:00, 67.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/demucs/hybrid_transformer/d12395a8-e57c48e6.th\" to /root/.cache/torch/hub/checkpoints/d12395a8-e57c48e6.th\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80.2M/80.2M [00:00<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/demucs/hybrid_transformer/92cfc3b6-ef3bcb9c.th\" to /root/.cache/torch/hub/checkpoints/92cfc3b6-ef3bcb9c.th\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80.2M/80.2M [00:00<00:00, 141MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/demucs/hybrid_transformer/04573f0d-f3cf25b2.th\" to /root/.cache/torch/hub/checkpoints/04573f0d-f3cf25b2.th\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 80.2M/80.2M [00:00<00:00, 129MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avvio restauro campioni...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:   3%|\u258e         | 1/30 [00:02<01:02,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: str_Strumenti__violin_sonata.wav\n",
            "Loaded audio shape: torch.Size([1, 222080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 222080])\n",
            "Enhance input shape: torch.Size([2, 222080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 222080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:   7%|\u258b         | 2/30 [00:03<00:41,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 222080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 222080])\n",
            "Combined shape after sum: torch.Size([2, 222080])\n",
            "Combined mono shape: torch.Size([222080])\n",
            "Final result shape: torch.Size([1, 222080])\n",
            "Enhanced shape before save: torch.Size([1, 222080])\n",
            "Processato e salvato: mus_Musica_con_female_pop_vocal_catchy_chorus.wav\n",
            "Loaded audio shape: torch.Size([1, 190080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 190080])\n",
            "Enhance input shape: torch.Size([2, 190080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 190080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  10%|\u2588         | 3/30 [00:04<00:33,  1.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 190080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 190080])\n",
            "Combined shape after sum: torch.Size([2, 190080])\n",
            "Combined mono shape: torch.Size([190080])\n",
            "Final result shape: torch.Size([1, 190080])\n",
            "Enhanced shape before save: torch.Size([1, 190080])\n",
            "Processato e salvato: ens_Ensemble_m_folk_band_fiddle_banjo_guitar.wav\n",
            "Loaded audio shape: torch.Size([1, 318080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 318080])\n",
            "Enhance input shape: torch.Size([2, 318080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 318080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  13%|\u2588\u258e        | 4/30 [00:06<00:39,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 318080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 318080])\n",
            "Combined shape after sum: torch.Size([2, 318080])\n",
            "Combined mono shape: torch.Size([318080])\n",
            "Final result shape: torch.Size([1, 318080])\n",
            "Enhanced shape before save: torch.Size([1, 318080])\n",
            "Processato e salvato: dis_Distorti_overcompressed_pop_song_with_a.wav\n",
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  17%|\u2588\u258b        | 5/30 [00:07<00:41,  1.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: gen_Generi_ele_house_music_piano.wav\n",
            "Loaded audio shape: torch.Size([1, 222080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 222080])\n",
            "Enhance input shape: torch.Size([2, 222080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 222080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  20%|\u2588\u2588        | 6/30 [00:09<00:34,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 222080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 222080])\n",
            "Combined shape after sum: torch.Size([2, 222080])\n",
            "Combined mono shape: torch.Size([222080])\n",
            "Final result shape: torch.Size([1, 222080])\n",
            "Enhanced shape before save: torch.Size([1, 222080])\n",
            "Processato e salvato: str_Strumenti__jazz_piano_solo.wav\n",
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  23%|\u2588\u2588\u258e       | 7/30 [00:10<00:37,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: eff_Effetti_spaceship_engine_and_scifi_eff.wav\n",
            "Loaded audio shape: torch.Size([1, 222080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 222080])\n",
            "Enhance input shape: torch.Size([2, 222080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 222080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  27%|\u2588\u2588\u258b       | 8/30 [00:11<00:31,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 222080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 222080])\n",
            "Combined shape after sum: torch.Size([2, 222080])\n",
            "Combined mono shape: torch.Size([222080])\n",
            "Final result shape: torch.Size([1, 222080])\n",
            "Enhanced shape before save: torch.Size([1, 222080])\n",
            "Processato e salvato: dis_Distorti_vinyl_record_with_heavy_scratc.wav\n",
            "Loaded audio shape: torch.Size([1, 222080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 222080])\n",
            "Enhance input shape: torch.Size([2, 222080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 222080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  30%|\u2588\u2588\u2588       | 9/30 [00:12<00:26,  1.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 222080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 222080])\n",
            "Combined shape after sum: torch.Size([2, 222080])\n",
            "Combined mono shape: torch.Size([222080])\n",
            "Final result shape: torch.Size([1, 222080])\n",
            "Enhanced shape before save: torch.Size([1, 222080])\n",
            "Processato e salvato: str_Strumenti__classical_guitar_piece.wav\n",
            "Loaded audio shape: torch.Size([1, 222080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 222080])\n",
            "Enhance input shape: torch.Size([2, 222080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 222080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  33%|\u2588\u2588\u2588\u258e      | 10/30 [00:13<00:23,  1.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 222080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 222080])\n",
            "Combined shape after sum: torch.Size([2, 222080])\n",
            "Combined mono shape: torch.Size([222080])\n",
            "Final result shape: torch.Size([1, 222080])\n",
            "Enhanced shape before save: torch.Size([1, 222080])\n",
            "Processato e salvato: eff_Effetti_forest_with_animals_and_stream.wav\n",
            "Loaded audio shape: torch.Size([1, 318080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 318080])\n",
            "Enhance input shape: torch.Size([2, 318080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 318080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  37%|\u2588\u2588\u2588\u258b      | 11/30 [00:15<00:26,  1.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 318080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 318080])\n",
            "Combined shape after sum: torch.Size([2, 318080])\n",
            "Combined mono shape: torch.Size([318080])\n",
            "Final result shape: torch.Size([1, 318080])\n",
            "Enhanced shape before save: torch.Size([1, 318080])\n",
            "Processato e salvato: mus_Musica_con_male_rock_vocal_gritty.wav\n",
            "Loaded audio shape: torch.Size([1, 286080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 286080])\n",
            "Enhance input shape: torch.Size([2, 286080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 286080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  40%|\u2588\u2588\u2588\u2588      | 12/30 [00:17<00:28,  1.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 286080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 286080])\n",
            "Combined shape after sum: torch.Size([2, 286080])\n",
            "Combined mono shape: torch.Size([286080])\n",
            "Final result shape: torch.Size([1, 286080])\n",
            "Enhanced shape before save: torch.Size([1, 286080])\n",
            "Processato e salvato: eff_Effetti_epic_battle_scene_with_swords_.wav\n",
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  43%|\u2588\u2588\u2588\u2588\u258e     | 13/30 [00:19<00:28,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: gen_Generi_ele_drum_and_bass_fast.wav\n",
            "Loaded audio shape: torch.Size([1, 158080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 158080])\n",
            "Enhance input shape: torch.Size([2, 158080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 158080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  47%|\u2588\u2588\u2588\u2588\u258b     | 14/30 [00:20<00:23,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 158080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 158080])\n",
            "Combined shape after sum: torch.Size([2, 158080])\n",
            "Combined mono shape: torch.Size([158080])\n",
            "Final result shape: torch.Size([1, 158080])\n",
            "Enhanced shape before save: torch.Size([1, 158080])\n",
            "Processato e salvato: eff_Effetti_rainstorm_with_thunder_and_win.wav\n",
            "Loaded audio shape: torch.Size([1, 222080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 222080])\n",
            "Enhance input shape: torch.Size([2, 222080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 222080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  50%|\u2588\u2588\u2588\u2588\u2588     | 15/30 [00:21<00:20,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 222080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 222080])\n",
            "Combined shape after sum: torch.Size([2, 222080])\n",
            "Combined mono shape: torch.Size([222080])\n",
            "Final result shape: torch.Size([1, 222080])\n",
            "Enhanced shape before save: torch.Size([1, 222080])\n",
            "Processato e salvato: str_Strumenti__saxophone_improvisation.wav\n",
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 16/30 [00:23<00:19,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: gen_Generi_ele_techno_beat_130_bpm.wav\n",
            "Loaded audio shape: torch.Size([1, 158080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 158080])\n",
            "Enhance input shape: torch.Size([2, 158080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 158080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 17/30 [00:24<00:16,  1.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 158080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 158080])\n",
            "Combined shape after sum: torch.Size([2, 158080])\n",
            "Combined mono shape: torch.Size([158080])\n",
            "Final result shape: torch.Size([1, 158080])\n",
            "Enhanced shape before save: torch.Size([1, 158080])\n",
            "Processato e salvato: ens_Ensemble_m_string_quartet_classical.wav\n",
            "Loaded audio shape: torch.Size([1, 190080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 190080])\n",
            "Enhance input shape: torch.Size([2, 190080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 190080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:25<00:14,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 190080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 190080])\n",
            "Combined shape after sum: torch.Size([2, 190080])\n",
            "Combined mono shape: torch.Size([190080])\n",
            "Final result shape: torch.Size([1, 190080])\n",
            "Enhanced shape before save: torch.Size([1, 190080])\n",
            "Processato e salvato: dis_Distorti_heavily_distorted_guitar_with_.wav\n",
            "Loaded audio shape: torch.Size([1, 286080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 286080])\n",
            "Enhance input shape: torch.Size([2, 286080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 286080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:27<00:15,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 286080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 286080])\n",
            "Combined shape after sum: torch.Size([2, 286080])\n",
            "Combined mono shape: torch.Size([286080])\n",
            "Final result shape: torch.Size([1, 286080])\n",
            "Enhanced shape before save: torch.Size([1, 286080])\n",
            "Processato e salvato: dis_Distorti_8bit_video_game_music_with_int.wav\n",
            "Loaded audio shape: torch.Size([1, 286080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 286080])\n",
            "Enhance input shape: torch.Size([2, 286080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 286080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:29<00:15,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 286080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 286080])\n",
            "Combined shape after sum: torch.Size([2, 286080])\n",
            "Combined mono shape: torch.Size([286080])\n",
            "Final result shape: torch.Size([1, 286080])\n",
            "Enhanced shape before save: torch.Size([1, 286080])\n",
            "Processato e salvato: mus_Musica_con_RB_soulful_female_vocal.wav\n",
            "Loaded audio shape: torch.Size([1, 286080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 286080])\n",
            "Enhance input shape: torch.Size([2, 286080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 286080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:30<00:14,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 286080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 286080])\n",
            "Combined shape after sum: torch.Size([2, 286080])\n",
            "Combined mono shape: torch.Size([286080])\n",
            "Final result shape: torch.Size([1, 286080])\n",
            "Enhanced shape before save: torch.Size([1, 286080])\n",
            "Processato e salvato: ens_Ensemble_m_jazz_trio_piano_bass_drums.wav\n",
            "Loaded audio shape: torch.Size([1, 286080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 286080])\n",
            "Enhance input shape: torch.Size([2, 286080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 286080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:32<00:14,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 286080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 286080])\n",
            "Combined shape after sum: torch.Size([2, 286080])\n",
            "Combined mono shape: torch.Size([286080])\n",
            "Final result shape: torch.Size([1, 286080])\n",
            "Enhanced shape before save: torch.Size([1, 286080])\n",
            "Processato e salvato: ens_Ensemble_m_rock_band_guitar_drums_bass.wav\n",
            "Loaded audio shape: torch.Size([1, 318080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 318080])\n",
            "Enhance input shape: torch.Size([2, 318080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 318080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:34<00:12,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 318080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 318080])\n",
            "Combined shape after sum: torch.Size([2, 318080])\n",
            "Combined mono shape: torch.Size([318080])\n",
            "Final result shape: torch.Size([1, 318080])\n",
            "Enhanced shape before save: torch.Size([1, 318080])\n",
            "Processato e salvato: mus_Musica_con_opera_tenor_aria_powerful.wav\n",
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:36<00:11,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: gen_Generi_ele_chillout_downtempo.wav\n",
            "Loaded audio shape: torch.Size([1, 190080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 190080])\n",
            "Enhance input shape: torch.Size([2, 190080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 190080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [00:37<00:07,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 190080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 190080])\n",
            "Combined shape after sum: torch.Size([2, 190080])\n",
            "Combined mono shape: torch.Size([190080])\n",
            "Final result shape: torch.Size([1, 190080])\n",
            "Enhanced shape before save: torch.Size([1, 190080])\n",
            "Processato e salvato: ens_Ensemble_m_marching_band_brass_percussion.wav\n",
            "Loaded audio shape: torch.Size([1, 254080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 254080])\n",
            "Enhance input shape: torch.Size([2, 254080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 254080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [00:39<00:06,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 254080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 254080])\n",
            "Combined shape after sum: torch.Size([2, 254080])\n",
            "Combined mono shape: torch.Size([254080])\n",
            "Final result shape: torch.Size([1, 254080])\n",
            "Enhanced shape before save: torch.Size([1, 254080])\n",
            "Processato e salvato: mus_Musica_con_rap_verse_with_beatbox.wav\n",
            "Loaded audio shape: torch.Size([1, 318080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 318080])\n",
            "Enhance input shape: torch.Size([2, 318080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 318080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [00:41<00:05,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 318080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 318080])\n",
            "Combined shape after sum: torch.Size([2, 318080])\n",
            "Combined mono shape: torch.Size([318080])\n",
            "Final result shape: torch.Size([1, 318080])\n",
            "Enhanced shape before save: torch.Size([1, 318080])\n",
            "Processato e salvato: dis_Distorti_low_quality_radio_broadcast_wi.wav\n",
            "Loaded audio shape: torch.Size([1, 158080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 158080])\n",
            "Enhance input shape: torch.Size([2, 158080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 158080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [00:42<00:02,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 158080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 158080])\n",
            "Combined shape after sum: torch.Size([2, 158080])\n",
            "Combined mono shape: torch.Size([158080])\n",
            "Final result shape: torch.Size([1, 158080])\n",
            "Enhanced shape before save: torch.Size([1, 158080])\n",
            "Processato e salvato: eff_Effetti_busy_city_street_with_traffic_.wav\n",
            "Loaded audio shape: torch.Size([1, 190080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 190080])\n",
            "Enhance input shape: torch.Size([2, 190080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 190080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing audio:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:43<00:01,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 190080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 190080])\n",
            "Combined shape after sum: torch.Size([2, 190080])\n",
            "Combined mono shape: torch.Size([190080])\n",
            "Final result shape: torch.Size([1, 190080])\n",
            "Enhanced shape before save: torch.Size([1, 190080])\n",
            "Processato e salvato: str_Strumenti__acoustic_bass_groove.wav\n",
            "Loaded audio shape: torch.Size([1, 158080]), sample rate: 32000\n",
            "Audio shape after preprocessing: torch.Size([2, 158080])\n",
            "Enhance input shape: torch.Size([2, 158080])\n",
            "Shape before apply_demucs: torch.Size([1, 2, 158080])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing audio: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:44<00:00,  1.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sources shape after apply_demucs: torch.Size([1, 4, 2, 158080])\n",
            "Sources shape after squeeze batch: torch.Size([4, 2, 158080])\n",
            "Combined shape after sum: torch.Size([2, 158080])\n",
            "Combined mono shape: torch.Size([158080])\n",
            "Final result shape: torch.Size([1, 158080])\n",
            "Enhanced shape before save: torch.Size([1, 158080])\n",
            "Processato e salvato: gen_Generi_ele_dubstep_wobbly_bass.wav\n",
            "\n",
            "Completata elaborazione. File in output directory: 30\n",
            "\n",
            "============================================================\n",
            "VALUTAZIONE RISULTATI - TUTTI I CAMPIONI\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "str_Strumenti__violin_sonata.wav (Categoria: Strumenti):\n",
            "  \u2022 iSNR finale : 41.97 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.01x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "mus_Musica_con_female_pop_vocal_catchy_chorus.wav (Categoria: Musica):\n",
            "  \u2022 iSNR finale : 5.74 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.86x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.91x\n",
            "\n",
            "ens_Ensemble_m_folk_band_fiddle_banjo_guitar.wav (Categoria: Ensemble):\n",
            "  \u2022 iSNR finale : 42.34 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.99x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "dis_Distorti_overcompressed_pop_song_with_a.wav (Categoria: Distorti):\n",
            "  \u2022 iSNR finale : 10.15 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.08x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.95x\n",
            "\n",
            "gen_Generi_ele_house_music_piano.wav (Categoria: Generi):\n",
            "  \u2022 iSNR finale : 25.96 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.99x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "str_Strumenti__jazz_piano_solo.wav (Categoria: Strumenti):\n",
            "  \u2022 iSNR finale : 12.18 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.52x\n",
            "  \u2022 Miglioramento contrasto spettrale: 1.00x\n",
            "\n",
            "eff_Effetti_spaceship_engine_and_scifi_eff.wav (Categoria: Effetti):\n",
            "  \u2022 iSNR finale : 11.73 dB\n",
            "  \u2022 Ratio energia alte frequenze: 2.40x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "dis_Distorti_vinyl_record_with_heavy_scratc.wav (Categoria: Distorti):\n",
            "  \u2022 iSNR finale : 5.32 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.24x\n",
            "  \u2022 Miglioramento contrasto spettrale: 1.00x\n",
            "\n",
            "str_Strumenti__classical_guitar_piece.wav (Categoria: Strumenti):\n",
            "  \u2022 iSNR finale : 34.00 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.92x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "eff_Effetti_forest_with_animals_and_stream.wav (Categoria: Effetti):\n",
            "  \u2022 iSNR finale : 39.53 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.96x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "mus_Musica_con_male_rock_vocal_gritty.wav (Categoria: Musica):\n",
            "  \u2022 iSNR finale : 15.53 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.81x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.97x\n",
            "\n",
            "eff_Effetti_epic_battle_scene_with_swords_.wav (Categoria: Effetti):\n",
            "  \u2022 iSNR finale : 11.81 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.70x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "gen_Generi_ele_drum_and_bass_fast.wav (Categoria: Generi):\n",
            "  \u2022 iSNR finale : 20.34 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.00x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "eff_Effetti_rainstorm_with_thunder_and_win.wav (Categoria: Effetti):\n",
            "  \u2022 iSNR finale : 3.84 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.24x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "str_Strumenti__saxophone_improvisation.wav (Categoria: Strumenti):\n",
            "  \u2022 iSNR finale : 42.38 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.00x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "gen_Generi_ele_techno_beat_130_bpm.wav (Categoria: Generi):\n",
            "  \u2022 iSNR finale : 17.53 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.02x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.95x\n",
            "\n",
            "ens_Ensemble_m_string_quartet_classical.wav (Categoria: Ensemble):\n",
            "  \u2022 iSNR finale : 28.73 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.88x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.94x\n",
            "\n",
            "dis_Distorti_heavily_distorted_guitar_with_.wav (Categoria: Distorti):\n",
            "  \u2022 iSNR finale : 14.35 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.65x\n",
            "  \u2022 Miglioramento contrasto spettrale: 1.00x\n",
            "\n",
            "dis_Distorti_8bit_video_game_music_with_int.wav (Categoria: Distorti):\n",
            "  \u2022 iSNR finale : 7.86 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.04x\n",
            "  \u2022 Miglioramento contrasto spettrale: 1.02x\n",
            "\n",
            "mus_Musica_con_RB_soulful_female_vocal.wav (Categoria: Musica):\n",
            "  \u2022 iSNR finale : 17.43 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.99x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.95x\n",
            "\n",
            "ens_Ensemble_m_jazz_trio_piano_bass_drums.wav (Categoria: Ensemble):\n",
            "  \u2022 iSNR finale : 25.05 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.62x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "ens_Ensemble_m_rock_band_guitar_drums_bass.wav (Categoria: Ensemble):\n",
            "  \u2022 iSNR finale : 20.64 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.02x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.97x\n",
            "\n",
            "mus_Musica_con_opera_tenor_aria_powerful.wav (Categoria: Musica):\n",
            "  \u2022 iSNR finale : 40.64 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.98x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "gen_Generi_ele_chillout_downtempo.wav (Categoria: Generi):\n",
            "  \u2022 iSNR finale : 44.02 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.09x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.97x\n",
            "\n",
            "ens_Ensemble_m_marching_band_brass_percussion.wav (Categoria: Ensemble):\n",
            "  \u2022 iSNR finale : 22.81 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.66x\n",
            "  \u2022 Miglioramento contrasto spettrale: 1.00x\n",
            "\n",
            "mus_Musica_con_rap_verse_with_beatbox.wav (Categoria: Musica):\n",
            "  \u2022 iSNR finale : 14.46 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.03x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.97x\n",
            "\n",
            "dis_Distorti_low_quality_radio_broadcast_wi.wav (Categoria: Distorti):\n",
            "  \u2022 iSNR finale : 19.93 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.86x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "eff_Effetti_busy_city_street_with_traffic_.wav (Categoria: Effetti):\n",
            "  \u2022 iSNR finale : 0.45 dB\n",
            "  \u2022 Ratio energia alte frequenze: 1.37x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.98x\n",
            "\n",
            "str_Strumenti__acoustic_bass_groove.wav (Categoria: Strumenti):\n",
            "  \u2022 iSNR finale : 15.47 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.83x\n",
            "  \u2022 Miglioramento contrasto spettrale: 0.99x\n",
            "\n",
            "gen_Generi_ele_dubstep_wobbly_bass.wav (Categoria: Generi):\n",
            "  \u2022 iSNR finale : 16.93 dB\n",
            "  \u2022 Ratio energia alte frequenze: 0.69x\n",
            "  \u2022 Miglioramento contrasto spettrale: 1.01x\n",
            "\n",
            "------------------------------------------------------------\n",
            "MEDIA SU 30 CAMPIONI:\n",
            "  \u2022 SNR migliorato medio: 20.97 dB\n",
            "    \u2022 Ratio HF medio: 1.01x\n",
            "    \u2022 Miglioramento contrasto spettrale medio: 0.98x\n",
            "\n",
            "------------------------------------------------------------\n",
            "MEDIE PER CATEGORIA:\n",
            "\n",
            "  Strumenti:\n",
            "    \u2022 SNR migliorato medio: 29.20 dB\n",
            "    \u2022 Ratio HF medio: 1.06x\n",
            "    \u2022 Miglioramento contrasto medio: 0.99x\n",
            "\n",
            "  Musica:\n",
            "    \u2022 SNR migliorato medio: 18.76 dB\n",
            "    \u2022 Ratio HF medio: 0.93x\n",
            "    \u2022 Miglioramento contrasto medio: 0.96x\n",
            "\n",
            "  Ensemble:\n",
            "    \u2022 SNR migliorato medio: 27.91 dB\n",
            "    \u2022 Ratio HF medio: 0.83x\n",
            "    \u2022 Miglioramento contrasto medio: 0.98x\n",
            "\n",
            "  Distorti:\n",
            "    \u2022 SNR migliorato medio: 11.52 dB\n",
            "    \u2022 Ratio HF medio: 1.17x\n",
            "    \u2022 Miglioramento contrasto medio: 0.99x\n",
            "\n",
            "  Generi:\n",
            "    \u2022 SNR migliorato medio: 24.96 dB\n",
            "    \u2022 Ratio HF medio: 0.96x\n",
            "    \u2022 Miglioramento contrasto medio: 0.98x\n",
            "\n",
            "  Effetti:\n",
            "    \u2022 SNR migliorato medio: 13.47 dB\n",
            "    \u2022 Ratio HF medio: 1.13x\n",
            "    \u2022 Miglioramento contrasto medio: 0.99x\n",
            "\n",
            "============================================================\n",
            "INTERPRETAZIONE RISULTATI:\n",
            "============================================================\n",
            "\u2022 iSNR > 0 dB: Riduzione del rumore\n",
            "\u2022 Ratio HF > 1.0: Miglioramento alte frequenze\n",
            "\u2022 Contrasto > 1.0: Miglioramento chiarezza strumentale\n",
            "\n",
            "Ascolta i campioni in:\n",
            "  Input: /content/raw_samples\n",
            "  Output: /content/restored_samples\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}